# üîß Fixes Applied for 400 Error

## What Was Wrong

The 400 error with Groq (and other AI providers) was caused by:

1. **Invalid Model Names** - Some model names were outdated or incorrect
2. **Excessive Max Tokens** - Exceeding provider limits (Groq has 6000 limit)
3. **Poor Error Messages** - You couldn't see the actual API error details
4. **No Diagnostics** - No way to test if your API key was working

## What I Fixed

### 1. ‚úÖ Fixed Model Names & Token Limits

**For Groq:**
- Capped `max_tokens` to 6000 (was 8000)
- Added validation for model name
- Added detailed logging

**For Claude:**
- Fixed model name from `claude-3-5-sonnet-20241022` to `claude-3-5-sonnet-20240620`
- Capped max_tokens to 8000

**For OpenAI:**
- Fixed model name from `gpt-4-turbo-preview` to `gpt-4-turbo`
- Added proper model selection based on provider

### 2. ‚úÖ Added Better Error Handling

Now when an error occurs, you'll see:
- The HTTP status code (400, 401, etc.)
- The full error response from the API
- Detailed logs in the developer console
- Which provider and settings were used

### 3. ‚úÖ Added Console Logging

Every API call now logs:
```
[AI Provider] Provider: groq-llama-3.1
[AI Provider] API Key present: true/false
[AI Provider] Max Tokens: 6000
[Groq] Using model: llama-3.1-70b-versatile, max_tokens: 6000
```

### 4. ‚úÖ Added API Test Command

**NEW COMMAND:** `QA Test Generator: Test AI API Connection`

This command:
- Tests your API key without generating test cases
- Shows exact error messages from the API
- Validates your configuration
- Helps diagnose issues quickly

## üìã What You Need to Do NOW

### Step 1: Compile the Fixed Code

```bash
cd /Users/rpandit/Documents/Hackathon/QA_Workflow_automation
npm run compile
```

### Step 2: Reload VS Code Extension

1. Press `F5` to reload the extension in development mode
   OR
2. In VS Code, press `Cmd+Shift+P` ‚Üí "Developer: Reload Window"

### Step 3: Verify Your Groq Settings

Open VS Code Settings (`Cmd+,`) and check:

```
‚úÖ qaTestGenerator.ai.provider = "groq-llama-3.1"
‚úÖ qaTestGenerator.ai.apiKey = "gsk_..." (your actual key)
‚úÖ qaTestGenerator.ai.maxTokens = 6000 (or less)
‚úÖ qaTestGenerator.ai.temperature = 0.7
```

### Step 4: Test Your Connection

1. Open Command Palette: `Cmd+Shift+P`
2. Type: "Test AI API Connection"
3. Run the command
4. You should see: "‚úÖ Groq API connection successful!"

If it fails, you'll see the exact error message!

### Step 5: Check Your API Key

**Your Groq API key should:**
- Start with `gsk_`
- Have no spaces before or after
- Be from https://console.groq.com

**If your key is wrong:**
1. Go to https://console.groq.com
2. Create a new API key
3. Copy it (it starts with `gsk_`)
4. Paste in VS Code settings: `qaTestGenerator.ai.apiKey`

### Step 6: View Detailed Logs

If still having issues, check console logs:

1. Open Developer Console: `Cmd+Option+I` (Mac) or `Ctrl+Shift+I` (Windows)
2. Click "Console" tab
3. Look for messages starting with `[Groq]` or `[AI Provider]`
4. These show exactly what's being sent to the API

## üéØ Quick Troubleshooting

### If you see "No API key configured"
‚û°Ô∏è Add your API key in settings: `qaTestGenerator.ai.apiKey`

### If you see "Invalid API Key" (401)
‚û°Ô∏è Your API key is wrong. Get a new one from console.groq.com

### If you see "max_tokens is too large" (400)
‚û°Ô∏è Set `qaTestGenerator.ai.maxTokens` to 6000 or less

### If you see "Model not found" (400)
‚û°Ô∏è Don't set a custom model, or use: `llama-3.1-70b-versatile`

### If you see "Rate limit exceeded" (429)
‚û°Ô∏è Wait 30 seconds and try again (free tier has limits)

## üìÅ Files Changed

1. **src/services/aiProviderManager.ts**
   - Fixed all provider configurations
   - Added better error handling
   - Added API test methods
   - Added detailed logging

2. **src/extension.ts**
   - Added `testAIConnection` command

3. **package.json**
   - Added test command to command palette

4. **GROQ_TROUBLESHOOTING.md** (NEW)
   - Comprehensive troubleshooting guide

## üîç Example Error Output (Before vs After)

### Before (Unhelpful):
```
AI generation failed. Falling back to template-based generation. 
Error: AxiosError: Request failed with status code 400
```

### After (Detailed):
```
AI generation failed. Falling back to template-based generation.
Error: Groq API error (400): {
  "error": {
    "message": "max_tokens is too large. Maximum allowed is 6000",
    "type": "invalid_request_error"
  }
}
```

Plus console logs:
```
[AI Provider] Provider: groq-llama-3.1
[AI Provider] API Key present: true
[AI Provider] Max Tokens: 8000
[Groq] Using model: llama-3.1-70b-versatile, max_tokens: 6000
[Groq] API Error: 400 { "error": { ... } }
```

## üöÄ Next Steps

1. ‚úÖ Compile the code: `npm run compile`
2. ‚úÖ Reload VS Code extension
3. ‚úÖ Verify settings (provider, API key, max tokens)
4. ‚úÖ Run "Test AI API Connection" command
5. ‚úÖ Check the test passes
6. ‚úÖ Try generating test cases again

## üìû Still Not Working?

1. Read: `GROQ_TROUBLESHOOTING.md`
2. Check console logs for detailed errors
3. Verify Groq service status: https://status.groq.com
4. Try the template-based provider (always works, no API needed)

## üéâ Common Success Path

Most 400 errors are fixed by:
1. ‚úÖ Ensuring API key starts with `gsk_`
2. ‚úÖ Setting max_tokens to 6000
3. ‚úÖ Selecting provider as `groq-llama-3.1`
4. ‚úÖ No custom model specified

Good luck! üöÄ









